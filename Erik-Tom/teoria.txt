INTELLIGENZA ARTIFICIALE 
    ramo dell'informatica che sviluppa sistemi capaci di simulare funzioni cognitive simili a quelle umane
        - pensare 
        - apprendere 
        - risolvere problemi

ORIGINE DELL'IA 
    - anni 40 - 50 
        - idea di macchine pensanti prende una forma scientifica grazie ad 
            ALAN TURING

        - 1950 -- test di turing 
            propone un criterio per determinare se una macchina possa esibire un comportamento intelligente indistinguibile da quello umano

    - 1956 
        durante il workshop di DARTMOUTH viene cognato ufficialmente il termine INTELLIGENZA ARTIFICIALE  

            la sfida era quella di dimostrare che ogni aspetto dell'apprendimento e caratteristica dell' intelligenza umana potesse essere oggetto di simulazione da parte di una macchina 
        
    - anni 50-60   
        sviluppo dei primi programmi capaci di 
            - giocare a dama 
            - imparare a risolvere problemi logici 
            - comprendere semplici frasi 
    
    - 1966 -- creato ELIZA un programma che simulava conversazioni con uno psicoterapeuta 

    - anni 70 
        le aspettative eccessive si scontrano con la realta' 
        i problemi sono molto piu' complessi del previsto, la potenza di calcolo e' limitata 


IA CLASSICA 
    IA simbolica si basava sull'idea che il pensiero umano potesse essere replicato attraverso la manipolazione di simboli secondo regole logiche formali 

        PRO 
            capacita' di spiegare il proprio ragionamento (seguendo regole) utili in domini ben definiti e stabili 
        
        CONTRO 
            - FRAGILITA' -- inefficaci al di fuori del loro ristretto dominio 
            - COSTO -- difficili e costosi da costruire e aggiornare "il collo di bottiglia della conoscenza "
            - RIGIDITA' -- faticano a gestire l'incertezza / ambiguita' / e il "buon senso" comune

    
    - dagli anni 80 ad OGGI 
        aumento dei dati e ritorno di interesse per l'IA 
        l'IA si concentra su compiti specifici catturando la conoscenza di esperti umani in un dominio ristretto 
        successi commerciali ma sistemi fragili e difficili da aggiornare 

    - anni 90 - 2000
        cambio di paradigma, non si programmano piu' le regole esplicite ma si creano algoritmi che imparano dai dati 
            nascono le RETI NEURALI 

DEEP BLUE vs KASPAROV 
    nel 1996 il computer DEEP BLUE di IBM sconfisse il campione del mondo di scacchi GARRY KASPAROV
    
    anche se erano algoritmi specifici per lo scacchi si nota l'aumento incredibile della potenza di calcolo delle macchine 


successi in diversi campi grazie a 
    - grandi quantita' di dati 
    - potenza di calcolo enormemente aumentata (GPU)
    - Deep Neural Network 

il concorso di riconoscimento di immagini IMAGENET nel 2012 fu vinto da un team che utilizzava una rete neurale profonda (alexNet)
superando drasticamente le prestazioni dei metodi precendenti 

RUOLO DELL'IA NELLA CYBERSECURITY

L'AI ha un doppio ruolo nella CYBERSECURITY
    - OFFENSIVO 
        utilizzato dagli attaccanti grazie all'assenza di vincoli legali ed etici, consente l'industrializzazione degli attacchi 

    - DIFENSIVO
        consente una difesa autonoma, una risposta agli incidenti adattiva e un contenimento basato sull'apprendimento automatico 


CRIME AS A SERVICE 
    intelligenza artificiale generativa ha industrializzato il crimine informatico attraverso il CRIME AS A SERVICE CaaS 
    e' un modello di business in cui i criminali informatici vendono o affittano i propri strumenti, servizi e competenze dannosi ad altri individui sul mercano nero 

    LLM come GPT-4 e le loro versioni dannose (WormGPT, FraudGPT) hanno rivoluzionato gli attacchi di phishing 
        - Spear-phishing su larga scala 
            creazione di migliaia di e-mail uniche e personalizzate che imitano lo stile di scrittura del destinatario 
        
        - Contesto perfetto 
            le e-mail possono fare riferimento a eventi recenti, colleghi reali p progetti interni incrementando la credibilita' 
        
        - bypass dei filti 
            la natura unica di ogni e-mail rende inefficaci i filti basati sulle firme 
    
    MALWARE 2.0 
        - Malaware polimorfico 
            l'IA consente al malaware di riscrivere il proprio codice ad ogni infezione, rendendo inutili le firme statiche 
        
        - fuzzing IA per 0-day 
            l' IA viene utilizzata per "stressare " in modo intelligente le applicazioni inviando input anomali per scoprire nuove vulnerabilita' (0-day) molto piu' rapidamente rispetto ai metodi tradizionali 
        
        - Creazione di exploit 
            gli LLM sono addestrati per analizzare il codice sorgente o binari, identificare vulnerabilita e scrivere automaticamente codice per sfruttarle 
    
    OSINT 
        l'attaccante usa l'intelligenza artificiale per raccogliere informazioni su un obbiettivo (azienda/sito web/ individuo ) senza intervento umano 

        - scansione continua 
            monitora GitHub, Linkedin e forum per indentificare codici / credenziali o nomi di dipendenti trapelati 
        
        - shadow IT Discovery
            identifica server dimenticati o API non protette 
        
        - Priorita' degli obbiettivi 
            individua le vulnerabilita e le classifica in base alla probabile importanza del sistema 
    

GOOGLE THREAT REPORT 
    - Elusione delle misure di sicurezza 
        gli autori delle minacce utilizzano tecniche di ingegneria sociale per indurre Gemini a fornire informazioni riservate 
    
    - Malware Just-in-time 
        utilizza modelli di LLM durante l'esecuzione per generare dinamicamente codice dannoso ed eludere il rilevamento 


IA in CYBERSECURITY ?
    - Volume eponenziale delle Minacce 
        ogni giorno nascono milioni di nuove varianti di malware, email di phishing, tentativi di attacco 
    
    - Complessita' dell'infrastruttura 
        reti sono enormi e intricate --> superfice d'attacco molto ampia 
    
    - Velocita' degli attacchi 
        metodi di difesa tradizionali sono troppo lenti a reaggire a minacce nuove o modificate 
    
    - Alert Fatigue 
        sistemi di sicurezza generano una quantita' immensa di log e alert 

    - Rilevamento e Protezione PROATTIVA 
        - Anomaly detection --- segnala automaticamente attivita' insolite o sospette 
        - Analisi Malware --- rileva nuovi malware utilizzando il codice 
        - phishing detection --- analizza e rileva email o sitiweb fraudolenti 
    
    - Gestione Predittiva delle vulnerabilita 
        analizza le vulnerabilita note e predice quali hanno la maggior probabilita di essere sfruttate dalle azinde 
    
    - Automazione della risposta (SOAR)
        automatizza compiti relativi e time-consuming come le analisi degli allert 
    
    - Analisi comportamentale 
        modella il comportamento tipico degli utenti e rileva anomalie che potrebbero indicare un account compromesso o una minaccia interna 
    

ANOMALY DETECTION 
    e' il processo di identificazione di dati / eventi o osservazioni che si discostano significativamente dal comportamento normale o atteso di un sistema 
        soluzione piu' comune IDS (intrusion detection system )
    
    IDS
        - Signature based -- identifica attivita' sospette confrontando il traffico di rete o l'attivita' del sistema con un database di signature predefinite 
        - Anomaly Based -- rileva attacchi sconosciuti rilevando comportamenti anomali non precedentemente classificati come minacce 

            esistono diverse ricerche che utilizzano IDS Anomaly based con i quali si ottengono buone prestazioni per rilevare anomalie e attacchi  
    
        ALERT FILTERING 
            alto numero di falsi positivi -- gli IDS spesso classificano erroneamente attivita' legittime ma insolite come dannose a causa delle diversita' e dell'imprevedibilita' del comportamento di rete 
            PROBLEMI 
                - aumento del carico di lavoro e casi di bornout tra gli analisti SOC che filtrano manualmente i falsi positivi 
                - tempo per l'analisi manuale 
                - Rallenta il processo di risposta agli attacchi 
        
                Possibile approccio: filtraggio automatico basato sull'intelligenza artificiale degli altri dopo averli identificati tramite IDS.
        
        MALWARE DETECTION 
            processo di identificazione e classificazione di software dannosi come virus / trojan o ransomware per proteggere i sistemi 
            
            TIPI DI MALWARE DETECTION
                - Signature-based -- utilizza indicatori digitali noti di malware per identificare comportamenti sospetti 
                - Analisi statica dei File -- esamina il codice nel file senza eseguirlo per identificare i segni di intenzioni dannose 
                - Analisi dinamica del Malware -- esegue codice sospetto in un ambiente sicuro chiamato sandbox 
                - AI-based -- apprende modelli di dati esistenti per prevedere risposte su nuovi dati 

        PHISHING DETECTION 
            gli LLM sono utili per rilevare il phishing in modo efficiente ed efficace 
            generando prompt per gli LLM in base alle informazioni del corpo e dell'intestazione dell'email 



Definizioni 
    - Intelligenza Artificiale:
         tecnicheche consentono alle macchine di imitare il comportamento umano.
    - Machine Learning:
        sistemi che imparano dai dati utilizzando dei metodi statistici per fare predizioni.
    - Deep Learning:
        usano le reti neurale per elaborare i dati in profondità e trovare pattern complessi.


Gli algoritmi di Machine Learning sono spesso classificati in base al modo in cui un algoritmo impara a diventare piu' accurato nelle sue supervisioni 
    Principali tipologie di ML sono 
        - Supervised Learning
            modello viene addestrato su insiemi di dati etichettati, ogni esempio di addestramento e' composta dalla coppia input e output
                TIPI 
                    > CLASSIFICAZIONE -- assegna dei dati di input a categorie o classi predefinite 
                    > REGRESSIONE -- identifica la relazione piu' adatta tra input e output 
        
        - Unsupervised Learning
            analizza dati non etichettati per scoprirne schemi nascosti o strutture intrinseche con l'obbiettivo di identificare relazioni interne al dataset 
        
        - Semi-Supervised Learning
            combina una piccola quantita di dati etichettati con una grande quantita di dati non etichettati durante l'addestramento
                TIPI 
                    > Self-Training -- il modello viene inizialmente addestrato sui dati etichettati e poi prevede le etichette per i dati non etichettati 
                    > CO-Training -- due o piu modelli vengono addestrati simultaneamente su viste diverse degli stessi dati 

        - Reinforcement Learning
            un agente impara a prendere decisioni interagendo con un ambiente 
                l'agente riceve feedback sotto forma di ricompense o penalita' in base alle sue azioni, con obbiettivo di massimizzare le ricompense cumulative nel tempo 


SUPERVISED LEARNING 
    FEATURE (indicate con X )
        sono le caratteristiche che descrivono un oggetto o un fenomeno 
            da questo modello apprendono i pattern per realizzare le predizioni 
    
    TARGET o LABEL (indicata con y)
        e' la risposta che vogliamo ottenere, E' cio che stiamo cercando di prevedere usando le FEATURE


    PIPELINE 

        - ACQUISIZIONE DATI 
            fase in cui si raccolgono i dati grezzi che serviranno per addestrare i modelli per realizzare le analisi le fonti possono essere 
                database / sensori / File / API / Web Scraping 
        
        - VALUTAZIONE DATI 
            fase di visualizzazione dei dati con scopo di mostrarli in modo chiaro attraverso rappresentazione geometrica 
                utile per :
                    > identificare pattern o trend --- scoprire relazioni nascoste / andamenti temporali   
                    > individuare anomalie --- individuare valori strani (outliner) / errori di inserimento o dati mancanti 
                    > Guidare il PREPROCESSING --- capire quali pulizie e trasformazioni saranno necessarie nella fase successiva 

        - PREPROCESSING 
            fase di pulizia e preparazione dei dati prima di darli all'algoritmo 
                i dati possono essere : incompleti / incosistenti  / Rumorosi / nel formato sbagliato 

        - MODELLI 
            fase di scelta dell'algoritmo piu' adatto al tipo di problema e alla natura dei dati 
                tipi di problemi 
                    > classificazione --- prevede una categoria  
                    > regressione  --- prevede un valore numerico 
        
        - METRICHE DI VALUTAZIONE
            fase di misurazione delle statistiche dei modelli addestrati verificando quanto bene generalizza su dati nuovi mai visti prima 
                Metriche che ci interessano in base al problema 
                    > classificazione --- accuracy, precision, recall, f1-score o matrice di confusione
                    > regressione --- Mean Squared Error (MSE), Root MSE (RMSE) o R-squared (R2)


COME RACCOGLIERE DATI 
    - RACCOLTA DIRETTA 
        vantaggi -- specificita' / controllo qualita'
        svantaggi -- Costo / Tempo 
            FONTI 
                - sondaggi e questionari online 
                - interviste
                - dati pubblici --- ISTAT / enti di ricerca / organizzazioni internazionali 
                - dati aziendali interni --- registri di vendita / log di sistema 
                - Web Scraping / API --- estrazione automatica di dati da siti web utilizzando API pubbliche 
                - Sensori IOT 

    - RACCOLTA da DATI GIA' ESISTENTI 
        vantaggi -- Costo-efficacia / disponibilita' immediata / vasta gamma 
        svantaggi -- rilevanza e qualita' non sempre garantita 



FORMATI DEI DATI 
    i dati possono avere diversi formati 
        - Strutturati 
            organizzati in formato tabellare con righe e colonne 
                - database 
                - Excel 
                - Customer Relationship Management (CRM)

        - Semi-Strutturati
            parzialmente organizzati con tag o marcatori 
                - XML 
                - JSON 
                - Email 

        - Non Strutturati
            Senza formato predefinito, richiedono elaborazione 
                - testi liberi 
                - immagini 
                - video e Audio 
                - Log e Serie storiche  

    FORMATO CSV 
        - testo semplice 
        - dati separati da virgole (o altri delimitatori)
        - ogni riga e' un record, ogni colonna un campo 
        - ideale per fogli di calcolo e scambi rapidi 
    
    FORMATO JSON 
        - formato semi-strutturato 
        - Organizzato in coppie chiave-valore e array 
        - Leggibile dall' uomo e dalla macchina 
        - standard per le API web e i database NoSQL 
    
    FORMATO XML 
        - formato strutturato basato su tag 
        - Definisce la struttura dei dati con elementi e attributi 
        - storicamente usato per lo scambio dati tra sistemi 


VISUALIZZAZIONE DATI 
    tipi di visualizzazione 
        - GRAFICO a BARRE -- confrontare categorie 
        - GRAFICO a TORTA -- parti di un intero 
        - GRAFICO a LINEE -- andamenti nel tempo 
        - MAPPA -- dati geografici 

ANALISI UNIVARIATA 
    analisi che si concentra su una variabile alla volta 

    ISTOGRAMMA
        perfetto per dati numerici  (eta' / prezzo / temperatura )
        
        Permette di vedere la distribuzione 
            - media 
            - varianza 
            - Skewness 
    
    GRAFICO A BARRE 
        per dati categorici (colore / tipo di prodotto / citta')
        utile per verificare il bilanciamento delle classi 
    
    GRAFICO A LINEE 
        collega i punti per mostrare l'andamento dei dati che evolovono nel tempo 
            - trend --  direzione generale dei dati a lungo termine 
            - Stagionalita' -- pattern che si ripetono ciclicamente 
            - Rumore -- variazioni casuali che non protano informazione 
    
    SCATTER PLOT 
        mette a confronto due variabili numeriche sugli assi X e Y
            - Correlazione -- se X sale, Y sale? -- (utile per la REGRESSIONE )

            - Cluster -- dati si raggruppano naturalmente ? -- (utile per la classificazione )
    

    BOX PLOT 
        IQR -- rappresenta il 50% centrale dei dati
            comprende dal 1*quartile (25%) al 3*quartile (75%), piu' e' alto piu' i dati sono dispersi 
        
            - linea MEDIANA (Q2) -- divide esattamente a meta' i dati 
                                    se non e' al centro della scatola allora la distribuzione e' asimmetrica 
            
            - BAFFI (Whiskers) -- indicando il range di dati considerati "normali"
            - outliner -- dati che cadono oltre i baffi, sono anomalie statistiche da investigare 
    

    HEATMAP 
        quando abbiamo molte feature non usiamo la heatmap 

        il colore indica l'intensita' della relazione
            questo ci aiuta nella feature selection 
            se due variabili dicono la stessa cosa (relazione = 1 ) possiamo eliminarne una  per semplificare il modello 




PREPROCESSING 
    i dati devono essere ACCURATI - COMPLETI - COERENTI 
    i dati sono di solito  
        - rumorosi  -- contenenti errori o valori ce si discostano da quelli previsti 
        - Incompleti -- cioe' privi di valori di funzionalita' 
        - incoerenti -- con discrepanze 
        - Campioni duplicati 
    
    PULIZIA DEI DATI 
        - inserimento valori mancanti 
        - identificazione e rimozione dei valori animali 
        - risoluzione delle incongruenze 

        POSSIBILITA'
            - ELIMINARE IL CAMPIONE -- feature con molti attributi con valori mancanti 
            - INSERIRE VALORI MANCANTI MANUALMENTE -- time-consuming
                opzioni  
                    - costante globale per inserire i valori mancanti 
                    - sostituire i valori con media o mediana   
                        - Media -- calcola la loro somma e la divide per il numero totale di valori 
                                    rappresenta un valore centrale o di equilibrio della distribuzione ma molto sensibile agli outliner
                        
                        - Mediana -- valore centrale in un insieme di dati ordinati 
                                    se i dati sono dispari e' il valore di centro 
                                    se invece sono pari e' la media aritmetica dei due valori centrali 
    
    ENCODING DEI DATI CATEGORICI 
        quasi tutti i modelli di ML elaborano solamente dati numerici, ma i dataset contengono spesso feature categoriche (testuali)
        TIPI DI DATI CATEGORICI

            - DATI NOMINALI
                    le categorie non hanno un ordine intrinseco o gerarchico, ma l'ordine e' arbitrario 
                    esempio (colori / nazioni)   
            - DATI ORDINALI  
                    le categorie hanno un ordine intrinseco. c'e' una relazione magiore e minore tra le categorie 
        
        METODOLOGIE DI ENCODING 
            
            LABEL ENCODING -- assegna un nuero intero univoco a ciascuna categoria 
                
                PRO 
                    - semplice da implementare 
                    - non aumenta il numero di colonne 
                
                CONTRO 
                    introduce una relazione d'ordine artificiale che puo' confondere i dati sono nominali.
                    adatto principalmente per dati ordinali dove l'ordine e' numerico rispecchia l'ordine reale o per la variabile target in problemi di classificazione 
            

            ONE HOT ENCODING -- crea una nuova colonna binaria per ogni categoria unica presente nella feature originale 
                
                PRO 
                    non introduce alcun ordine artificiale, quindi ideale per i dati nominali 
                
                CONTRO
                    puo' portare a un aumento significativo del numero di colonne se la feature originale ha molte categorie uniche rende il modello piu' complesso e lento 
                
            
            BINARY ENCODING -- viene assegnato ad ogni categoria un numero intero viene covenrtito nella sua rappresentazione binaria e le cifre binarie vengono separate in nuove colonne distinte 
                
                PRO    
                    - riduzione della Dimensionalita' -- crea molte meno colonne rispetto il one-hot ENCODING, specialmente per le feature con tante categorie 
                    - evita l'ordine artificiale diretto -- non impone la relazione lineare semplice che si crea con il label encoding 
                
                CONTRO
                    - interpretabilita' ridotta -- le nuove colonne binarie non hanno un significato diretto e intuitivo come il One Hot encoding
                    - potenziale difficolta' per i modelli -- alcuni modelli potrebbero avere difficolta'  a decifrare l'informazione codificata in formato binario multicolonna 
                
        
    
L'obbiettivo dell' IA e' costruire dei modelli che non solo imparino dai dati forniti, ma che riescano a generalizzare bene 
    PROBLEMA 
        valutare il modello sull'intero dataset a disposizione su cui e' anche addestrato il modello e' ingannevole.
        in questo modo il modello "impara a memoria" (OVERFITTING) su quei dati e fallisce se ci sono dati nuovi 

    SOLUZIONE 
        simulano lo scenario in cui arrivano i nuovi dati, dividendo il dataset a disposizione in 3 porzioni prima di iniziare il Training
            le porzioni sono 
                - training set
                    utilizzato dal modello durante l'addestramento per imparare pattern / relazioni e regolare i propri parametri interni (pesi e bias )
                    il modello vede ripetutamente questi dati (spesso per molte epoche ) e aggiusta i suoi parametri per minimizzare l'errore su questo specifico set 
                        SOLITAMENTE costituisce la porzione piu' GRANDE circa 70 / 80 %

                - validation set
                    utilizzato durante la fase di sviluppo e tuning del modello 
                        - tuning degli iperparametri 
                            sceglie i valori migliori per le impostazioni del modello 
                                si prova una configurazione, si valuta sul validation test, si aggiusta e riprova

                        - Monitoraggio dell'overfitting 
                            controlla le performance del modello su dati nuovi durante l'addestramento

                        - selezione del modello 
                            se si confrontano diversi modelli il validation test aiuta a scegliere quello che generalizza meglio 
                                SOLITAMENTE costituisce una porzione di circa 10 / 15 %

                - test set  
                    ultima porzione di dati che viene utilizzato solo alla fine di tutto i processo 
                        fornisce una valutazione finale e imparziale delle prestazioni del modello usato 
                            SOLITAMENTE costituisce una porzione del 10 / 15 % 
            
    
DATASET SBILANCIATO 

IL BILANCIAMENTO VA APPLICATO SOLO AL TRAIN SET 

    PROBLEMA    
        il modello impara a predirre quasi sempre la classe piu' frequente, ignorando o performando male sulla classe minoritaria 
    
    STRATEGIE 
        - OVERSAMPLING -- aumenta il numero di campioni della classe minoritaria 
            - randomOverSampling -- duplicazione casuale 
            - Synthetic Minority Over-Sampling Technique (SMOTE) -- crea nuovi campioni sintetici simili a quelli esistenti 

        - UNDERSAMPLING -- riduce il numero di campioni della classe maggioritaria 
            - RANDOM UNDERSAMPLING -- rimozione casuale dei sample 
        
        - APPROCCI IBRIDI -- combinazione di OVERSAMPLING e UNDERSAMPLING 
            - Cost-Sensitive Learning  
                assegna un costo di penalita maggiore agli errori di classificazione sulla classe minoritaria 
                molti algoritmi come SVM/Alberi Decisionali / Regressione Logistica 
                permettono di impostare pesi per le classi 


NORMALIZZAZIONE 
    e' una tecnica di pre-elaborazione dei dati utilizzata per trasformare i dati in modo che tutti abbiano una scala comune senza distorizioni 
        -- EVITA DOMINANZA DELLE VARIABILI - variabili con scale maggiori non influenza eccessivamente il modello 
        -- RIDUCE SENSIBILITA' AGLI OUTLINER -- rende il modello meno vulnerabile ai valori anomali 
        -- MIGLIORA CONVERGENZA -- accellera l'apprendimento e stabilizza i modelli basati su gradiente 

    - MIN-MAX SCALING 
        traforma i dati in un intervallo specifico di solito [0,1] o [-1,1]
            viene utilizzato quando si conosce l'intervallo dei dati e si vuole mantenere la interpretabilita' delle variabili 
                CARATTERISTICHE     
                    - Preserva la forma originale della distribuzione dei dati 
                    - molto sensibile agli outlier. Se c'e' un valore estremamente grande (outliner)
                        questo divetera' l'1 e schiaccera' tutti gli altri valori nominali verso lo 0 rendendo i dati poco distinguibili 
    
    - STANDARD SCALER 
        trasforma i dati in modo che abbiano una MEDIA (μ) pari a 0 e una DEVIAZIONE STANDARD (σ) pari a 1. processo noto come calcolo dello Z-score 

        CARATTERISTICHE 
            - tende a rendere i dati piu' simili a una distribuzione normale (Gaussiana)
            - e' influenzato dagli outliner (perche' influenzano media e deviazione standard ) ma e' considerato piu' robusto del MIN-MAX perche' non comprime forzatamente tutti i dati in un intervallo minuscolo a causa di un intervallo minuscolo a causa di un singolo valore esterno 


    LA NORMALIZZAZIONE viene applicata sia su training che test set ma calcolata solo sul training 
        perche bisogna trattare il test set come se fosse dati futuri di cui non si conosce ancora la distribuzione
        STEP 
            - calcola i parametri sul Training SET (fit): calcola la media e la deviazione standard (per lo standard) o minimo - massimo  (per il MIN MAX ) utilizzando solo i dati del Training Set 
            - Applica la trasformazione al Training Set (trasform): utilizza quei parametri per scalare i dati di training 
            - applica la stessa trasformazione al test set (trasform): utilizza i parametri calcolati al punto 1 per scalare il test set. non ricalcola nulla sul test set 
        
    CHE TIPO DI NORMALIZZAZIONE SCEGLIER E
        - STANDARD 
            - l'algoritmo che usi assume che i dati abbiano una distribuzione normale 
            - i dati hanno outliner che non puoi eliminare 
        
        MIN-MAX 
            - uso RETI NEURALI che spesso preferisco input 0-1 per le funzioni di attivazione 
            - l'algoritmo non fa assunzioni sulla distribuzione dei dati 
            - se i dati sono sparsi e si vuole mantenere tale struttura 


TRAINING 
    e' il processo generale che dura per N epoche; ogni EPOCA consiste nel processare l'intero training set tramite molte iterazioni; ogni iterazione lavora su un BATCH per aggiornare il modello 
         - EPOCA --- passaggio completo del'algoritmo di addestramento attraverso l'intero training set 
                OBBIETTIOVO -- aggiustare progressivamente i parametri del modello per minimizzare la funzione di perdita 
        
        - BATCH (MINI - BATCH) --- un piccolo sottoinsieme di dati estratto dal training set, utilizzato per calcolare il gradiente e aggiornare i parametri 
                OBBIETTIVO -- calcolare una stima approssimata del gradiente e eseguire un singolo aggiornamento dei parametri 
        
        - ITERAZIONE --- azione di elaborare un singolo batch ed eseguire l'aggiornamento dei parametri. una iterazione equivale all'elaborazione di un batch 



STEP DI PREPARAZIONE DEI DATI PER IL MODELLO 
    1. Pulizia dati 
    2. Encoding dati categorici 
    3. Split in Train set e Test set 
    4. Bilanciamento delle classi solo per il train set se necessario 
    5. Normalizzazione 
    6. MODELLO 




MODELLI DI MACHINE LEARNING 

    LINEAR REGRESSION 
        modello traccia un grafico lineare tra due variabili di dati che separa meglio i dati 
            VANTAGGI 
                - predice valori continui (numeri)
                - Cerca una relazione "causa - effetto "\
                - molto veloce e facile da interpretare 
            
            SVANTAGGI 
                - sensibile ai valori anomali  (outliner )
                - Troppo semplice per relazioni complesse 
    
    SUPPORT VECTOR MACHINE (SVM)
        cerca l'iperpiano ottimale che massimizza il margine tra i punti dati piu' vicini 
        con obbiettivo di massimizzare il margine tra i punti per consentire all'algoritmo di individuare il miglio confine decisionale tra le classi 
    
        SVM non lineare: in molti scenari reali non sono separabili linearmente, USA il kernel trick 
            - kernel polinomiale 
            - kernel RBF (o gaussiano)

            VANTAGGI 
                - molto efficace in spazi ad alta dimensionalita' 
                - Utilizza il Kernel Trick per dati non lineari 
            
            SVANTAGGI 
                - non adatto a dataset molto grandi  (training molto lento )
                - calo di prestazioni se i dati si sovrapponogono troppo 
                - difficile scegliere il kernel giusto 

    
    DECISION TREE 
        il modello impara a dividere i dati ponendo domande sequenziali 
            ogni domanda riduce l'incertezza fino ad una decisione finale 
        
            - NODO RADICE -- punto di partenza in cui e' presente la prima domanda/codizione 
            - NODI INTERNI o DECISIONALI -- possibili risposte 
            - RAMI -- le connessioni tra i nodi che rappresentano i valori o le risposte possibili a una condizione posta dal nodo 
            - NODI FOGLIA -- rappresentano tutti i possibili risultati nel test set 

            VANTAGGI 
                - facilmente spiegabile -- rispecchiano i processi decisionali umani 
                - Visualizzabile graficamente -- facilmente interpretabile anche dai non addetti ai lavori 

            SVANTAGGI 
                - tendenza all'overfitting -- troppo specifico 
                - instabile -- piccoli cambiamenti nei dati alterano l'albero 
    
    RANDOM FOREST 
        e' un insieme di molti alberi decisionali che votano per la decisione finale 
            obbiettivo 
                - migliorare l'accuratezza della previsione 
                - ridurre il rischio di overfitting 
            
            FUNZIONAMENTO 
                - creazione di sottoinsiemi casuali 
                    > dal training set vengono creati sottoinsiemi di dati selezionando dei campioni 
                    > ogni sottoinsieme viene utilizzato per addestrare un singolo albero decisionale 
            
                - addestramento degli alberi con "casualita" aggiuntiva 
                    > durante la costruzione di ciascun albero ad ogni nodo viene considerata solo una selezione causale delle feature disponibili per decidere la divisione migliore 
                    > Questo passaggio (RANDOM SUBSPACE) assicura che gli alberi nella foresta siano diversificati e non troppo correlati tra loro 
                
                - aggregazione delle previsioni 
                    > classificazione -- ogni albero della foresta vota per una classe => la classe che riceve il maggior numero di voti e' la previsione finale del modello 
                    > regressione -- la previsione finale e' la media delle previsioni di tutti gli alberi della foresta 
            
            VANTAGGI 
                - alta accuratezza -- generalmente fornisce ottime prestazioni 
                - robustezza all'overfitting -- grazie alla selezione casuale delle feature e' meno incline all'overfitting rispetto ad un singolo albero decisionale complesso 
            
            SVANTAGGI 
                - minore interpretabilita -- e' piu' difficile interpretare il processo decisionale esatto rispetto a un singolo albero decisionale e capire perche' il modello ha fatto una certa previsione e' complesso 
                - costo computazionale -- richiede piu' risorse per l'addestramento rispetto a modelli piu' semplici dato che bisogna costruire piu' alberi 
        
    
    ADABOOST 
        combina piu' alberi singoli, ovvero modelli con prestazioni leggermente migliori rispetto al caso random per formare uno strong learner
        ogni weak learner viene addestrato in sequenza per correggere gli errori commessi dai modelli precedenti 
            dopo tante iterazioni i weak learner vengono trasformati in strong learner 

            VANTAGGI 
                - trasforma modelli deboli in modelli forti 
                - spesso raggiunge l'accuratezza piu' alta possibile 
                - meno inclini all'overfitting rispetto ai singoli alberi 

            SVANTAGGI 
                - lento da addestrare (e' sequenziale non parallelo )
                - sensibile agli outliner 
                - richiede una pulizia dati molto accurata 
    

    K-NEAREST NEIGHBORS (KNN)
        classifica un punto in base alla classe dei suoi k vicini piu' simili nello spazio 
            - calcolo delle distanze -- quando si presenta un nuovo sample da classificare o per cui si vuole fare una previsione si calcola la distanza tra questo sample e tutti i sample nel dataset di addestramento 
            - selezione dei k vicini -- una volta calcolate le distanze l'algoritmo seleziona i k esempi piu' vicini al nuovo esempio 
                                        il numero di k e' un iperparametro che deve essere scelto a priori 
                > k=1 -- confine decisionale e' eccessivamente flessibile 
                > mano a mano che il k cresce il metodo diventa meno flessibile e produce un limite di decisione che e' vicino alla linerarita' 
                > k piccolo produce molte piccole regioni di ogni classe mentre un k grande porta ad un minor numero di regioni piu' grandi 
                > richiede l'archiviazione del set di dati di training per intero, questo e' un calcolo costoso se il dataset e molto grande 
            
            VANTAGGI 
                - estremamente sempice e intuitivo 
                - non richiede addestramento (lazy learning )
                - si adatta facilemente a nuovi dati 
            
            SVANTAGGI 
                - molto lento nella fase di test (deve calcolare le distanze )
                - sensibile alla scala dei dati 
                - maledizione della dimensionalita' 


    K-means Clutering 
        algoritmo non supervisionato che raggruppa i dati in k cluster bssati sulla similarita' 
        
        obbiettivo e' dividere i dati in k gruppi 
            step iterativi
            - INIZIALIZZAZIONE -- si scelgono k punti a caso (i centroidi )
            - ASSEGNAZIONE -- ogni dato si unisce al centroide piu' vicino 
            - AGGIORNAMENTO -- ogni centroide si sposta al (CENTRO DI MASSA) del suo gruppo 
            - CONVERGENZA -- si ripete finche' i centri non smettono di muoversi 

            VANTAGGI 
                - semplice da scalare su dateset enormi
                - non necessita di etichettatura 
                - Efficace per segmentazione automatica 
            
            SVANTAGGI 
                - Bisogna specificare K in anticipo 
                - molto sensibile ai centri inziali dei cluster 
                - funziona male con cluster di forme irregolari 


IPERPARAMETRI e PARAMETRI 
    - iperparametri -- sono le configurazioni esterne impostate prima che inizi il processo di apprendimento influenzano il comportamento del modello e il modo in cui vengono appresi i parametri del modello 

    - parametri -- sono interni al modello e sono direttamente influenzati dai dati di addestramento attraverso algoritmi di ottimizzazione 

    IPERPARAMETRI

        lo spazio delle combinazioni possibili cresce in modo esponenziale 
            - con pochi iperparametri esistono gia' centinaia di combinazioni possibili 
            - man mano che il numero di iperparametri e valori da testare aumetna il processo diventa impraticabile 
            - ogni combinazione richiede un addestramento completo che puo' richiedere ore o giorni 
        
        STRATEGIE DI OTTIMIZZAZIONE 
            - GRIND SEARCH 
                viene definita una griglia di valori e il modello viene addestrato per ogni singola combinazione. metodo esaustivo ma inefficiente 
            
            - RANDOM SEARCH 
                viene campionato un numero fisso di combinazioni casuali, il che spesso risulta piu' efficiente 

PROCESSO DI ADDESTRAMENTO 
    1. definizione spazio parametri 
    2. selezione metodo di ricerca 
    3. esecuzione K-Fold Cross-Validation 
    4. Calcolo Punteggio Medio 
    5. Restituzione Parametri Migliori 


CROSS VALIDATION 
    e' una tecnica di validazione utilizzata per valutare la capacita' di un modello di generalizzare su dati nuovi evitando problemi come l'overfitting 

    non suddivide i dati una solo volta in training e test set ma esegue piu' suddivisioni, utilizzando di diverse porzioni del dataset per l'addestramento e la valutazione 

    la cross-validation misura quanto e' buono il modello (termometro)
    Grind Search e Random Search cercano gli iperparametri migliori 
    
    FUNZIONAMENTO 
        1. il dataset e' suddiviso in k sottoinsiemi di dimensioni uguali 
        2. per ogni iterazione uno dei fold e' usato come test set, mentre gli altri k-1 fold sono usati come training set 
        3. questo processo e' ripetutto k volte ogni volta usando un fold diverso come test set 
        4. si calcola la metrica di valutazione per ciascun fold 
        5. si ottiene una stima finale calcolando la media delle metriche su k fold 

BIAS-VARIANCE TRADE-OFF 
    l'obbiettivo e' riuscire ad ottenere un modello che funzioni bene non solo su dati che gia' conosce ma che 
        - impari i pattern e le relazioni significative dai dati di addestramento 
        - generalizzi efficacemente -- sia accurato e affidabile anche quando applicato a dati nuovi e mai visti prima 
    
    le metriche che ci permettono di osservare questi aspetti sono  
        - training Loss -- misura quanto il modello sbaglia sui dati di training 
        - validation loss -- misura quanto il modello sbaglia su un set di dati nuovi, non usati nell'addestramento e' l'indicatore piu' importante della CAPACITA' DI GENERALIZZAZIONE 
    il confronto tra queste due metriche aiuta a capire come sta imparando il modello 

    - BIAS -- rappresenta l'errore dovuto a assunzioni errate o troppo semplici del modello 
    - variance -- rappresenta la sensibilita' del modello alle piccole fluttuazioni del set di addestramento 
        OBBIETTIVO IDEALE -- LOW BIAS , LOW Variance 
            dove il modello e' abbastanza complesso da capire i dati ma abbastanza semplice da non farsi influenzare dal rumore 

UNDERFITTING 
    si verifica quando un modello troppo semplice per catturare la struttura sottostante dei dati di addestramento 
        il modello non riesce a imparare le relazioni tra le feature e la variabile target 
    CARATTERISTICHE 
        - il modello ha prestazioni scarse sia sul training set che sul validation e test set 
        - Presenta un bias elevato: fa assunzioni troppo forti e semplicistiche sulla natura dei dati 
        - La curva di apprendimento  mostra un errore elevato sia per il training che per la validazione e gli errori tendono a convergere a un valore alto 
    
    CAUSE 
        - modello troppo semplice 
        - addestramento insufficiente 
        - features non abbastanza informative o indatte 


OVERFITTING 
    quando un modello impara i dati di training troppo bene  catturando non solo i pattern ma ance rumore e fluttuazioni 

    CARATTERISTICHE
        - il modello ha prestazioni eccellenti sul trining set ma scarse sul validation e test set 
        - presenta una varianza elevata: e' molto sensibile alle piccole variazioni nei dati di addestramento 
        - la curva di apprendimento mostra un errore molto basso sul training set ma un errore molto piu' alto sul validation set 
    
    CAUSE 
        - modello troppo complesso 
        - numero insufficiente di dati di addestramento 
        - addestramento troppo lungo il modello inizia a imparare il rumore 
    
    POSSIBILI SOLUZIONI 
        - ottenere piu' dati di addestramento 
        - Early Stopping -- interrompere l'addestramento non appnea l'errore di validazione smette di migliorare o inizia a peggiorare 


BIAS-VAIRANCE TRADE-OFF
    obbiettivo di trovare il Bias-variance trade-off che si verifica quando il modello ha appreso con successo i pattern generali e significativi dal training set ignorando il rumore irrilevante 

    CARATTERISTICHE 
        - modello ha la complessita' giusta per il problema 
        - presenta un buon livello di varianza e bias in modo da minimizzare l'errore totale sui dati di validation e test set 
        - la curva del VALIDATION LOSS rimane relativamente vicina alla curva della TRAINING LOSS 
            non c'e' un grande divario tra le due 
    
    e' spesso un processo iterativo di sperimentazione bilanciamento tra 
        - complessita' del modello 
        - durata dell'addestramento --- numero di epoche 
        - uso del CROSS-VALIDATION --- permette di stimare come il modello si comportera' su dati nuovi e aiuta a scegliere la complessita' ottimale 


METRICHE DI VALUTAZIONE 
    le metriche di valutazione permettono di confrontare diversi modelli o diverse versioni dello stesso modello usando criteri quantitativi e oggetti 
    la scelta delle metriche piu' appropriate dipende dal tipo di problema che stiamo affrontando e dagli obbiettivi specifici 

    METRICHE DI CLASSIFICAZIONE 
        - ACCURACY --- calcola la proporzione di previsioni corrette sul totale delle previsioni 
            puo' essere molto ingannevole in caso di dataset sbilanciati 
        
        - MATRICE DI CONFUSIONE --- permette di capire non solo quante predizioni sono sbagliate ma anche che tipo di errori commette il modello 
            si basa su 
                > TP(true positive) -- previsti correttamente come positivi 
                > TN(true negative) -- previsti correttamente come negativi 
                > FP(false positive) -- previsti erroneamente come positivi 
                > FN(false negative) -- previsti erroneamente come negativi 
        
        - PRECISION --- misura quanto sono affidabili le previsioni positive, se ha un valore alto vuol dire che commette pochi FP 
        - RECALL --- misura la capacita di identificare tutti i casi positivi se ha un valore alto vuol dire che il modello commette pochi FN 
        - F1-SCORE --- indica la media armonica di PRECISION e RECALL 
                        fornisce un singolo valore che bilancia entrambe -- utile quando sia FP e FN sono considerati costosi e su cerca un buon compromesso tra PRECISION e RECALL 

    METRICHE DI REGRESSIONE 
        - MEAN ABSOLUTE ERROR (MAE) --- misura la media della differenza assoluta tra i valori predetti dal modello e i valori reali 
                                        indica la media di quanto le predizioni si discostano dai valori reali

        - MEAN SQUARED ERROR (MSE) --- misura la media del quadrato della differenza tra valori predetti e i valori reali  
                                        metrica usata come funzione di loss durante l'addestramento
        
        - ROOT MEAN SQUARED ERROR (RMSE) --- e' la radice quadrata dell'MSE 
                                                rappresenta una distanza media tra le previsioni e i valori reali 

        - R-SQUARED (R2) --- misura la proporzione della varianza totale della variabile target y che viene spiegata dal modello di regressione 
            - R2 = 1 -- modello spiega perfettamente la variabilita' dei dati 
            - R2 = 0 -- non spiega la variabilita' di quanto farebbe predirre semplicemente la media 



RETI NEURALI 
    sono modelli computazionali che traggono ispirazione dalla struttura e dal funzionamento delle reti di neuroni biologici presenti nel cervello umano 
        scopo -- imparare pattern e relazioni complesse direttamente dai dati senza essere programmate esplicitamente per ogni compito specifico 
    
    le reti neurali eccellono in compiti dove le regole tradizionali sono difficili da definire 
        - riconoscimento immagini 
        - riconoscimento vocale 
        - sistemi di raccomandazione 
        - elaborazione di linguaggio naturale 
        - guida autonoma 
    
    PERCETTRONE 
        e' il tipo piu' semplice di rete neurale costituito da un singolo neurone 

        la funzione prende decisioni valutanto i segnali in ingresso sommando i valori e applicando una funzione di attivazione 
            COMPONENTI 
                - input -- variabili che rappresentano i punti dati 
                - Pesi -- importanza assegnata a ciascun input 
                - Bias -- regola l'output per migliorare la precisione e flessibilita' 
                - funzione di attivazione (σ) -- determina l'output in base a una soglia 

    FUNZIONE DI ATTIVAZIONE 
        e' una funzione applicata all'output di un neurone per introdurre non linearita' e consentire alla rete neurale di apprendere relazioni complesse tra input e output 
        TIPI DI FUNZIONI DI ATTIVAZIONE
            - SIGMOIDE -- output 0,1
            - TANH -- output -1,1 centrato sullo 0
            - ReLU -- e' la piu' comunemente usata negli HIDDEN LAYER 
            - SOFTMAX -- solo per il livello di output (classificazione multiclasse ) / output = distribuzione di probabilita (somma 1)
        
    MULTILAYER PERCEPTRON (MLP) 
        e' un estensione del PERCETTRONE con uno o piu' HIDDEN LAYER che forma una rete neurale feedforward 
        capacita' di modellare relazioni non lineari per risolvere problemi complessi 
            
            - input layer -- riceve i dati iniziali, non esegue calcoli si limita a trasmettere i dati
                COMPITI 
                    > primo strato che riceve i dati grezzi 
                    > ogni neurone rappresenta una carattersitica o una variabile del set di dati  
                    > non vengono eseguiti calcoli , i dati vengono passati allo strato successivo 
                

            - HIDDEN LAYER -- uno o piu' livelli intermedi tra input e output -- qui avviene la maggior parte dell'elaborazione     
                                i neuroni di un livello di solito sono connessi a quelli del livello successivo 
                COMPITI 
                    > i neuroni applicano funzioni di attivazione per catturare modelli piu' complessi 
                    > trasforma i dati in rappresentazioni significative 
                    > consente l'apprendimento dai dati per migliorare le prestazioni 

            - OUTPUT LAYER -- produce il risultato finale della rete
                COMPITI 
                    > strato finale che produce le previsioni della rete 
                    > i neuroni rappresentano variabili di output come categorie o valori numerici 
                    > applica funzioni di attivazione per formattare gli output 


    STEP DI ADDESTRAMENTO DI UNA RETE NEURALE 
        - PREPARAZIONE DATI 
            > Raccolta e pulizia dei dati -- garantire la qualita'
        
        - FORWARD PROPAGATION
            > passare gli input attraverso la rete per ottenere output 
        
        - CALCOLO DELLA LOSS 
            > utilizza una funzione di loss per quantificare gli errori di previsione 
        
        PROBLEMA -- torvare i pesi migliori per il nostro problema 
            - BACKPROPAGATION
                > calcola i gradienti della perdita rispetto ciascun peso 
                > utilizza questi gradienti per aggiornare i pesi nella direzione opposta 
            
            - AGGIORNAMENTI DEI PESI 
                regola i pesi utilizzando algoritmo di ottimizzazione come STOCHSTIC GRADIENT DESCENT (SGD)
    

    BACKPROPAGATION
        processo di regolazione dei pesi in una rete neurale mediante propagazione dell'errore all'indietro dal livello di output al livello di input 

            1. FORWARD PROPAGATION -- calcola output utilizzando pesi attuali 
            2. calcolo della loss -- misura la loss utilizzando funzione di perdita 
            3. BACKWARD PROPAGATION -- calcolare il gradiente (derivate parziali) della perdita rispetto a ciascun peso 
            4. AGGIORNAMENTO DEI PESI -- regolare i pesi nella direzione opposta al gradiente per ridurre al minimo la loss 
    
    
    DISCESA DEL GRADIENTE 
        e' un algoritmo di ottimizzazione che usa le info della BACKPROPAGATION
            - aggiorna pesi e bias -- facendo piccoli passi nella direzione opposta al gradiente ovvero la direzione che riduce piu' velocemente la loss 
            - utilizza il learning rate per controllare la dimensione dei passi 
        obbiettivo trovare il punto con loss minore 

    LEARNING RATE
        e' un piccolo numero positivo che controlla la dimensione del passo ad ogni aggiornamento 
            - se e' troppo piccolo --- addestramento molto lento 
            - se e' troppo grande --- si potrebbe saltare il minimo => aumentare la perdita 
    

    STOCHASTIC GRADIENT DESCENT  (SGD)
        calcola il gradiente e aggiorna i parametri utilizzando un solo sample di addestramento alla volta 
        VANTAGGIO   
            - piu' veloce per singolo aggiornamento , aggiunge rumore che puo' aiutare a saltare minimi locali poco profondi 
        SVANTAGGIO 
            - le stime del gradiente sono molto rumorose , la convergenza potrebbe essere instabile e oscillante 
    
    MINI-BATCH GRADIENT DESCENT 
        calcola il gradiente e aggiorna i parametri utilizzando un piccolo sottoinsieme di dati alla volta 
        (approccio piu' utilizzato )
    
        VANTAGGI 
            - bilancia la velocita di SGD con la stabilita' del BATCH GD 
            - sfrutta l'efficienza del calcolo vettoriale/matriciale sulle GPU 

    ADAM 
        adatta l'aggiornamento per ogni parametro basandosi sulla storia recente sia della direzione (momentum) sia della magnitudine (scaling) dei suoi gradienti 
        obbiettivo di addestrare in modo piu' veloce e stabile 

            - MOMENTUM 
                utilizza una media mobile dei gradienti passati accelerando il movimento in direzioni consistenti diminuendo le oscillazioni 
            - SCALING ADATTIVO 
                calcola tassi i learning rate specifici e adattivi per ogni singolo parametro della rete 
                cio permetti di ridurre il learning rate per parametri con gradienti grandi/variabili e di aumentarlo per parametri con gradienti piccoli 
            
            VANTAGGI 
                - convergenza rapida -- spesso raggiunge buoni risultati in meno iterazioni/epoche rispetto a SGD 
                - Robustezza al learning rate -- meno sensibile alla scelta iniziale del learning rate globale 
                - Buone prestazioni di default -- i parametri interni di ADAM hanno valori predefiniti che funzionano bene su un'ampia gamma di problemi 
                - Efficienza -- computazionalmente valido e implementato in tutte le principali librerie 
            
            CONTRO 
                richiede un po piu' di memoria rispetto SGD per memorizzare le stime m e v 