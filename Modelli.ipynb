{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa43de",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "# Import delle librerie essenziali\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n",
    "dataset_path = Path('EdgeIIoT-dataset.csv')\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b94177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Supponiamo che X e y siano i tuoi dati sbilanciati\n",
    "print(f\"Distribuzione originale: {Counter(y)}\")\n",
    "\n",
    "# 1. Inizializzazione del campionatore\n",
    "# sampling_strategy='auto' pareggia le classi 1:1\n",
    "rus = RandomUnderSampler(random_state=42, replacement=False)\n",
    "\n",
    "# 2. Applicazione dell'undersampling\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "print(f\"Nuova distribuzione: {Counter(y_resampled)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df0d4a",
   "metadata": {},
   "source": [
    "ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "class_weights = {cls: 1.0/count for cls, count in zip(classes, counts)}\n",
    "sample_weights = np.array([class_weights[label] for label in y_train])\n",
    "\n",
    "\n",
    "# 3. Parametri AdaBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "ada_search = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid, cv=5, scoring='f1',n_jobs=-1)\n",
    "ada_search.fit(X_train, y_train,sample_weight=sample_weights)\n",
    "\n",
    "\n",
    "\n",
    "print(\"best accuracy\",random_search_ada.best_score_)\n",
    "print(ada_search.best_estimator_)\n",
    "\n",
    "\n",
    "best_ada = ada_search.best_params_\n",
    "\n",
    "ada = AdaBoostClassifier(**best_ada)\n",
    "ada.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predict continuous values\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "\n",
    "#matrice di confusione\n",
    "cm_ada = confusion_matrix(y_test, y_pred_ada)\n",
    "\n",
    "# Use unique values for labels\n",
    "labels = sorted(df['type'].unique())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_ada, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels)\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Matrice di Confusione - ada boost')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51278954",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definizione del modello e dei parametri da testare\n",
    "rf = RandomForestClassifier(class_weight='balanced',random_state=42)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Ricerca della migliore combinazione\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_rf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(f\"Migliori parametri RF: {grid_rf.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285bed8d",
   "metadata": {},
   "source": [
    "Rete Neurale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"üì¶ Librerie caricate con successo!\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚öôÔ∏è Device: {device}\")\n",
    "\n",
    "\n",
    "print(\"üìä DIVISIONE DATASET:\")\n",
    "print(f\"üèãÔ∏è Train:      {len(X_train):3d} campioni ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"‚úÖ Validation: {len(X_val):3d} campioni ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"üéØ Test:       {len(X_test):3d} campioni ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# Verifica distribuzione classi\n",
    "print(\"\\nüîç Distribuzione classi per set:\")\n",
    "for name, y_set in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    dist = [np.sum(y_set == i) for i in range(2)]\n",
    "    print(f\"{name:5}: {dist} ‚Üí {[d/len(y_set)*100 for d in dist]}\")\n",
    "\n",
    "\n",
    "# Conversione in tensori PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val) \n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.LongTensor(np.array(y_train, dtype=np.float32))\n",
    "y_val_tensor = torch.LongTensor(np.array(y_val, dtype=np.float32))  \n",
    "y_test_tensor =  torch.LongTensor(np.array(y_test, dtype=np.float32))  \n",
    "\n",
    "print(f\"\\nüîÑ TENSORI PYTORCH:\")\n",
    "print(f\"X_train: {X_train_tensor.shape}\")\n",
    "print(f\"X_val:   {X_val_tensor.shape}\")\n",
    "print(f\"X_test:  {X_test_tensor.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fix_random(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_random(42)\n",
    "\n",
    "val_dataloader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=y_val_tensor.shape[0])\n",
    "test_dataloader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=y_test_tensor.shape[0])\n",
    "\n",
    "def get_model(input_size=83, dept=3, hidden_size=64, dropout_prob=0.2):\n",
    "\n",
    "    # 1. Primo layer: Connette l'input al primo strato nascosto\n",
    "    model = [nn.Linear(input_size, hidden_size), nn.ReLU() ]\n",
    "    \n",
    "    # 2. Layer intermedi (Hidden): Vengono aggiunti in un ciclo in base a 'dept'\n",
    "    for i in range(dept):\n",
    "        model.append(nn.Linear(hidden_size, hidden_size)) # Connessione lineare\n",
    "        model.append(nn.ReLU())                           # Attivazione non lineare\n",
    "        model.append(nn.Dropout(dropout_prob))            # Regolarizzazione (Dropout)\n",
    "        \n",
    "    # 3. Output layer: Connette l'ultimo layer nascosto alle 3 classi finali\n",
    "    model.append(nn.Linear(hidden_size, 2))\n",
    "    \n",
    "    # nn.Sequential unisce la lista di layer in un unico modello ordinato\n",
    "    return nn.Sequential(*model)\n",
    "\n",
    "\n",
    "# --- GRID SEARCH SETUP ---\n",
    "hidden_size = [128, 256]          \n",
    "dropout_prob = [0.2, 0.3]         \n",
    "dept = [3, 4]                     \n",
    "batch_size = [16, 32]             \n",
    "learning_rate = [0.001, 0.01]     \n",
    "\n",
    "params = product(hidden_size, dropout_prob, dept, batch_size, learning_rate)\n",
    "\n",
    "# Calcoliamo quante combinazioni dovremo testare\n",
    "combinations = len(hidden_size)*len(dropout_prob)*len(dept)*len(batch_size)*len(learning_rate)\n",
    "print(\"Numero totale di configurazioni da testare: \", combinations)\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, device, hidden_size=3, dropout_prob=0.2, dept=2, epochs=100, batch_size=32, learning_rate=.001):\n",
    "    \"\"\"\n",
    "    Esegue il training di un singolo modello.\n",
    "    \"\"\"\n",
    "    # LOSS FUNCTION: Usiamo CrossEntropyLoss perch√© √® un problema di Classificazione \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # OPTIMIZER: Adam √® l'algoritmo che aggiorna i pesi per minimizzare l'errore\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Liste per salvare lo storico degli errori\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    # EARLY STOPPING:\n",
    "    # ferma il modello se smette di migliorare sul validation set.\n",
    "    best_model = None\n",
    "    best_loss = np.inf   # Inizializziamo con infinito\n",
    "    patience = 10        # Quante epoche aspettiamo se non migliora\n",
    "    patience_counter = 0\n",
    "\n",
    "    # --- CICLO DI TRAINING (EPOCHE) ---\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Mettiamo il modello in modalit√† training (abilita Dropout, ecc.)\n",
    "        model.train() \n",
    "\n",
    "        # Iteriamo sui batch di dati\n",
    "        for x, y in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device) \n",
    "            \n",
    "            optimizer.zero_grad()    # 1. Azzeriamo i gradienti precedenti\n",
    "            y_pred = model(x)        # 2. Forward pass (previsione)\n",
    "            loss = criterion(y_pred, y) # 3. Calcolo errore\n",
    "            loss.backward()          # 4. Backward pass (calcolo gradienti)\n",
    "            optimizer.step()         # 5. Aggiornamento pesi\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Salviamo la loss media di training per questa epoca\n",
    "        train_loss.append(epoch_loss / len(train_dataloader))\n",
    "\n",
    "        # --- VALIDATION ---\n",
    "        model.eval() # Modalit√† valutazione (disabilita Dropout)\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad(): # Disabilita calcolo gradienti (pi√π veloce, meno memoria)\n",
    "            for x, y in val_dataloader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                epoch_val_loss += loss.item()\n",
    "        val_loss.append(epoch_val_loss / len(val_dataloader))\n",
    "\n",
    "        # Stampa progressi ogni 10 epoche\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Train loss: {train_loss[-1]:.4f}, Val loss: {val_loss[-1]:.4f}, Time: {time.time()-epoch_start:.2f}s')\n",
    "\n",
    "        # --- EARLY STOPPING CHECK ---\n",
    "        # Se l'errore di validazione √® il pi√π basso visto finora, salviamo questo modello\n",
    "        if val_loss[-1] < best_loss:\n",
    "            best_loss = val_loss[-1]\n",
    "            best_model = copy.deepcopy(model) # Creiamo una copia del modello attuale\n",
    "            patience_counter = 0 # Resettiamo il contatore\n",
    "        else:\n",
    "            # Se non migliora, incrementiamo il contatore\n",
    "            patience_counter += 1\n",
    "            if patience_counter == patience:\n",
    "                # Se abbiamo aspettato troppo interrompiamo.\n",
    "                print(\"Early stopping...\") \n",
    "                break\n",
    "\n",
    "    print(\"Training terminato in {} epoche. Miglior validation loss: {}\".format(epoch+1, best_loss))\n",
    "\n",
    "    # Restituiamo il miglior modello trovato (non necessariamente l'ultimo)\n",
    "    return best_model, train_loss, val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model, test_dataloader, device):\n",
    "    \"\"\"\n",
    "    Valuta il modello finale sul Test Set per ottenere accuracy e predizioni.\n",
    "    \"\"\"\n",
    "    model.eval() # Importante: modalit√† valutazione\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x , y = x.to(device), y.to(device)\n",
    "            \n",
    "            output = model(x)\n",
    "            _, predicted = torch.max(output, 1) # Ottieni classe vincente\n",
    "            \n",
    "            y_pred.extend(predicted.cpu().tolist())\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            \n",
    "    return y_pred, y_true\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- CICLO PRINCIPALE GRID SEARCH ---\n",
    "# Qui proviamo ad allenare una rete diversa per ogni combinazione di iperparametri\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_config = None\n",
    "iter_count = 0 \n",
    "\n",
    "# Iteriamo su tutte le combinazioni generate\n",
    "for hidden_size, dropout_prob, dept, batch_size, learning_rate in params:\n",
    "    iter_count += 1\n",
    "    print(f'\\n--- Iterazione {iter_count}/{combinations} ---')\n",
    "    print(f'Configurazione: Hidden={hidden_size}, Drop={dropout_prob}, Dept={dept}, Batch={batch_size}, LR={learning_rate}')\n",
    "\n",
    "    # 1. Creiamo un NUOVO modello con questa specifica configurazione\n",
    "    model = get_model(X_train.shape[1], dept=dept, hidden_size=hidden_size, dropout_prob=dropout_prob)\n",
    "    \n",
    "    # Creiamo il DataLoader specifico (perch√© il batch_size cambia)\n",
    "    train_dataloader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Dizionario utile per salvare la configurazione corrente\n",
    "    config = {\n",
    "        'hidden_size': hidden_size,\n",
    "        'dropout_prob': dropout_prob,\n",
    "        'dept': dept,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "\n",
    "    # 2. Alleniamo il modello e otteniamo la versione migliore (grazie all'early stopping)\n",
    "    trained_model, train_loss, val_loss = train(model, train_dataloader, val_dataloader, device, **config)\n",
    "\n",
    "    # 3. Testiamo il modello allenato sul Test Set\n",
    "    y_pred, y_true = test_model(trained_model, test_dataloader, device)\n",
    "    test_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print(f'Test Accuracy: {test_acc:.4f} (Migliore attuale: {best_accuracy:.4f})')\n",
    "\n",
    "    # 4. Confronto: √à questo il modello migliore visto finora?\n",
    "    if test_acc > best_accuracy:\n",
    "        best_accuracy = test_acc\n",
    "        best_model = copy.deepcopy(trained_model) # Salviamo una copia\n",
    "        best_config = config\n",
    "        print(\"üèÜ NUOVO RECORD!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c26648",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best config: {best_config}')\n",
    "print(f'Best accuracy: {best_accuracy}')\n",
    "\n",
    "y_pred, y_true = test_model(best_model, test_dataloader, device)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f'Final Accuracy score: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esercizi_machinelearning (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
